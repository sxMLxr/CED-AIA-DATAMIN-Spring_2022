{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Workshop [Student]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-95493327be7007a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cf2e07ca28bffdf7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# you should be familiar with numpy from HW0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# we will use the iris dataset from sklearn.datasets\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cf3556d59a71da2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the iris dataset and translate to pandas dataframe\n",
    "iris_sk = datasets.load_iris()\n",
    "# Note that the \"target\" attribute is species, represented as an integer\n",
    "data = pd.DataFrame(data= np.c_[iris_sk['data'], iris_sk['target']],columns= iris_sk['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc5ebf9199674355",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check rows and columns\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sampling [Follow] (20 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a0dfb5bb877bd812",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q1: Stratified sampling\n",
    "In this part, you will be writing code to do stratified sampling. You should sample the *same number* of rows for each value of the given attribute.\n",
    "You can use only pandas library calls for this problem.\n",
    "\n",
    "**Hint**: You should read about the [split-apply-combine](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html) coding pattern in Pandas before starting this problem! In particular pay attention to the following:\n",
    "* [Splitting an object into groups](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#splitting-an-object-into-groups)\n",
    "* [Transformation](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#transformation)\n",
    "\n",
    "How could you collect a sample from each species, and then combine them?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best way to do stratified sampling is with the `groupby` function from Pandas. You will see more about `groupby` in Seminar 4. It allow us to split the data into groups and then apply a function to each. For example, below we get the mean sepal length of each target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('target')['sepal length (cm)'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `apply` function allows us to apply a function to each group, and combine the results. We can use this for _stratified_ sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8b35ef41b81547d1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "stratified_data = data.groupby('target').apply(lambda x: x.sample(n=5))\n",
    "# Show the stratified dataframe\n",
    "stratified_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try running the sampling procedure multiple times to see how the output is different each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **plot the data on a Histogram** to show that it is equally sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEGIN SOLUTION\n",
    "plt.hist(stratified_data[\"target\"])\n",
    "plt.show()\n",
    "# END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "stratified_sampling",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Checking to make sure that there are 5 of each type\n",
    "np.testing.assert_equal(sum(stratified_data[\"target\"] == 0),5)\n",
    "np.testing.assert_equal(sum(stratified_data[\"target\"] == 1),5)\n",
    "np.testing.assert_equal(sum(stratified_data[\"target\"] == 2),5)\n",
    "assert any([(data.iloc[i,:] == stratified_data.iloc[0,:]).all() for i in data.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1 (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-95493327be7007a3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Import all necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cf2e07ca28bffdf7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# you should be familiar with numpy from HW0\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# we will use the iris dataset from sklearn.datasets\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cf3556d59a71da2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the iris dataset and translate to pandas dataframe\n",
    "iris_sk = datasets.load_iris()\n",
    "# Note that the \"target\" attribute is species, represented as an integer\n",
    "data = pd.DataFrame(data= np.c_[iris_sk['data'], iris_sk['target']],columns= iris_sk['feature_names'] + ['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fc5ebf9199674355",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check rows and columns\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Sampling [Group] (20 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX1: Random Sampling\n",
    "Now you'll be exploring pandas built in sampling library. Check the `sample` [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Sample 30 rows from a dataframe **without** replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_30 = None\n",
    "sample_30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, try running the sampling procedure multiple times to see how the output is different each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **plot the data on a Histogram** to show that it is equally sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the distribution of the species (target attribute)\n",
    "# How evenly are the species distributed with random sampling?\n",
    "# Try running it again - are the results the same?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Sample 40 rows from a dataframe **with** replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_40 = None\n",
    "sample_40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you check how rows are remaining after the duplicates are dropped\n",
    "# It should be less than 40.\n",
    "# (However there's a decent chance it might not be since random is random)\n",
    "# (Run the sampling procedure multiple times just in case)\n",
    "print(\"Number of rows after duplicates are dropped\", sample_40.drop_duplicates().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop duplicates for the row\n",
    "assert(sample_40.drop_duplicates().shape[0] < 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Sometimes, when testing or profiling data mining algorithms, it's useful to keep the same data around for reproducabililty and to track down bugs in the algorithm. \n",
    "\n",
    "In most data mining libraries, you can **seed** your random process so that it \"randomly\" picks the same data everytime. Then, when you want your truly random data, you can take the seed parmater out.\n",
    "\n",
    "Try this out by sampling with the `random_state` paramater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_seeded = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it again multiple times, and notice if it changes or not. What if you change the `random_state` number?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Discretization [Follow] (20 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c30171bad3bafef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q2: Equal-Width discretization\n",
    "\n",
    "In the following exercises, you will using Pandas to discretize a defined numpy vector into equal-width bins. The $n$ bins should all be of size $(max - min) / n$. In pandas, if a value falls directly on a break, it defaults to the lower break. \n",
    "\n",
    "Here's a webpage that explains more about `cut` and `qcut` | https://pbpython.com/pandas-qcut-cut.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([1, 6, 13, 40, 56, 7, 23, 43])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a variable called `bin_5`, discritize `v` into 5 equal width bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_5 = pd.cut(v, bins=5, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.array_equal(bin_5, np.array([0, 0, 1, 3, 4, 0, 1, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the same data, now cut it into 3 bins of equal width. Store it in a variable called `bin_3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in here!\n",
    "# SOLUTION\n",
    "bin_3 = pd.cut(v, bins=3, labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.array_equal(bin_3, np.array([0, 0, 0, 2, 2, 0, 1, 2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Discretization [Group] (10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9c30171bad3bafef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### EX2: Equal-Depth discretization\n",
    "\n",
    "In the following exercises, you will using Pandas to discretize a defined numpy vector into equal-width bins. The $n$ bins should all be of size $(max - min) / n$. In pandas, if a value falls directly on a break, it defaults to the lower break. \n",
    "\n",
    "Here's a webpage that explains more about `cut` and `qcut` | https://pbpython.com/pandas-qcut-cut.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.array([1, 6, 13, 40, 56, 7, 23, 43])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a variable called `bin_4`, discritize `v` into 4 equal depth bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_4 = None\n",
    "bin_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.array_equal(bin_4, np.array([0, 0, 1, 2, 3, 1, 2, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the same data, now cut it into 2 bins of equal depth. Store it in a variable called `bin_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_2 = None\n",
    "bin_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dimensionality Reduction [Follow] (30 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: Motivating PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a little bit of data viz. There's 4 features, but we can really see in 3-dimensions. However, let's try plotting a 3D scatter plot to see what we can gleam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write code to produce a three-dimensional scatter plot using the sepal length, sepal width and petal width as dimensions, and color the data points according to the class attribute. Here is some [documentation on 3D scatterplots](https://matplotlib.org/stable/gallery/mplot3d/scatter3d.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(12, 9))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "for grp_name, grp_idx in data.groupby('target').groups.items():\n",
    "    y = data.iloc[grp_idx][\"sepal length (cm)\"]\n",
    "    x = data.iloc[grp_idx][\"sepal width (cm)\"]\n",
    "    z = data.iloc[grp_idx][\"petal width (cm)\"]\n",
    "    ax.scatter(x,y,z, label=grp_name)  # this way you can control color/marker/size of each group freely\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('sepal length (cm)')\n",
    "ax.set_xlabel('sepal width (cm)')\n",
    "ax.set_zlabel('petal width (cm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What patterns do you see in the data? How separable is the data?\n",
    "\n",
    "If we transformed the data into 1 dimension, do you think we could get similar seperability? What about 2?\n",
    "\n",
    "Now, we're going to perform a PCA on the Iris Dataset to see if we can gain any insight using dimensionality reduction.\n",
    "\n",
    "Do a **one-dimensional** PCA of the iris dataset, and then plot the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of our data\n",
    "X = iris_sk.data\n",
    "Y = iris_sk.target\n",
    "\n",
    "#Choose number of components\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "#Calculate PCA\n",
    "pca.fit(X)\n",
    "\n",
    "#Get PCA version of fitted data\n",
    "transformed_X = pca.transform(X)\n",
    "#transformed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.scatter(transformed_X[:, 0], np.zeros(len(X)), c = Y)\n",
    "#plt.scatter(transformed_X, np.zeros(len(X)), c = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret what this graph is saying in your own words. What parts of the data are seperable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do this with two dimensional PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of our data\n",
    "X = iris_sk.data\n",
    "Y = iris_sk.target\n",
    "\n",
    "#Choose number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "#Calculate PCA\n",
    "pca.fit(X)\n",
    "\n",
    "#Get PCA version of fitted data\n",
    "transformed_X = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.scatter(transformed_X[:, 0], transformed_X[:, 1], c = Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret what this graph is saying in your own words. Do we gain anymore information when we do a 2D PCA instead of 1D? Can we linearly seperate the green and yellow clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dimensionality Reduction [Group]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX3: Motivating PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting 1-D PCA\n",
    "Write your thoughts down below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpret what this graph is saying in your own words. Do we gain anymore information when we do a 2D PCA instead of 1D? Can we linearly seperate the green and yellow clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down your thoughts below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Data Visualization [Follow] (10 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to do some more data viz and EDA on the iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has an easy way to view all the important descriptive statistics of a dataset, called `describe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, **something doesn't make sense in the above diagram**. Do you know what's wrong?\n",
    "\n",
    "**ANSWER**: Trying to run summary statistics on target column, which is a nominal data type. Lesson to always understand your data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization [Group] (30 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX4.A: Boxplots\n",
    "Make a box-and-whisker plot for each feature (except the class attribute).\n",
    "\n",
    "Be sure to include a title for each plot of what feature is being described.\n",
    "\n",
    "[Making box-plots in pandas](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.boxplot.html)\n",
    "\n",
    "Most times, you can pass in the `figsize` paramater to any pandas plotting function in case the resulting graph is too small. E.g `plt.hist(data, figsize=(10,2)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION (can break it up into multipole steps if you want to) (E.g \"first we create a subset of the dataframe...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX4.B: Scatter Matrix\n",
    "A scatter matrix in an $n \\times n$ grid of scatterplots, which plots every feature against every other feature. Since the diagonal of the scatter matrix would be plotting a variable against itself, most libraries substitute the diagonal with a histogram distribution of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's [documentation for using scatter matrix using the pandas library](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html).\n",
    "\n",
    "(Remember that if your figure is too small, you can pass in a `figsize` paramater.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Distance Functions [Follow] (10 mins)\n",
    "\n",
    "Here, you will be learning how to manually implement distance functions.\n",
    "\n",
    "Here you will be getting a hang of distance functions in scipy. [List of distance functions in scipy](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5: Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial import distance\n",
    "iris_sk = datasets.load_iris()\n",
    "# let's scale the data to [0, 1] range to ensure that all the features are in the same range.\n",
    "iris_sk.data = MinMaxScaler().fit_transform(iris_sk.data)\n",
    "\n",
    "# we'll be using the iris_df dataframe for visualization later\n",
    "iris_df = pd.DataFrame(iris_sk.data, columns=iris_sk.feature_names)\n",
    "# add class variable. Now, the iris dataframe (iris_df) also incldues the nominal class variable indicating \n",
    "# the species for each data point. \n",
    "iris_df['Species'] = iris_sk.target\n",
    "\n",
    "# let's extract two random rows from the dataset. We'll use these rows for the distance metric calculations.\n",
    "p = iris_sk.data[10, :]\n",
    "q = iris_sk.data[50, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem a: Euclidean Distance\n",
    "\n",
    "def calculate_euclidean_distance(p , q):\n",
    "    \"\"\"\n",
    "    Input: p and q are two numpy vectors of same dimensions. \n",
    "    Output: a single floating point value contaning the euclidean \n",
    "            distance between p and q\n",
    "    \n",
    "    Allowed numpy functions: sum, square, sqrt\n",
    "    \"\"\"\n",
    "    \n",
    "    ## BEGIN SOLUTION\n",
    "    return np.sqrt(np.sum(np.square(p-q)))\n",
    "    ## END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_euclidean_distance(iris_sk.data[10, :], iris_sk.data[50, :])\n",
    "euclid_dist_10_50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function against the offical distance function implementations!\n",
    "np.testing.assert_almost_equal(calculate_euclidean_distance(iris_sk.data[10, :], iris_sk.data[50, :]), distance.euclidean(iris_sk.data[10, :], iris_sk.data[50, :]))\n",
    "np.testing.assert_almost_equal(calculate_euclidean_distance(iris_sk.data[20, :], iris_sk.data[30, :]), distance.euclidean(iris_sk.data[20, :], iris_sk.data[30, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Functions [Group] (20 mins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will be getting a hang of distance functions in scipy. [List of distance functions in scipy](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial import distance\n",
    "\n",
    "iris_sk = datasets.load_iris()\n",
    "# let's scale the data to [0, 1] range to ensure that all the features are in the same range.\n",
    "iris_sk.data = MinMaxScaler().fit_transform(iris_sk.data)\n",
    "\n",
    "# we'll be using the iris_df dataframe for visualization later\n",
    "iris_df = pd.DataFrame(iris_sk.data, columns=iris_sk.feature_names)\n",
    "# add class variable. Now, the iris dataframe (iris_df) also incldues the nominal class variable indicating \n",
    "# the species for each data point. \n",
    "iris_df['Species'] = iris_sk.target\n",
    "\n",
    "# let's extract two random rows from the dataset. We'll use these rows for the distance metric calculations.\n",
    "p = iris_sk.data[10, :]\n",
    "q = iris_sk.data[20, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX5.A: Cosine Distance\n",
    "Find the cosine distance between the 10th and 20th row in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neo4j.com/docs/graph-data-science/current/alpha-algorithms/cosine/\n",
    "def calculate_cosine_distance(p, q):\n",
    "    \"\"\"\n",
    "    Input: p and q are two numpy vectors of same dimensions. \n",
    "    Output: a single floating point value contaning the \n",
    "            cosine distance between p and q. \n",
    "    \n",
    "    \n",
    "    Allowed numpy functions: dot, sum, square, sqrt\n",
    "    \"\"\"\n",
    "\n",
    "    ## BEGIN SOLUTION\n",
    "    return None\n",
    "    ## END SOLUTION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_dist = None\n",
    "cos_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function against the offical distance function implementations!\n",
    "np.testing.assert_almost_equal(cos_dist, distance.cosine(iris_sk.data[10, :], iris_sk.data[20, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EX5.B: $L_\\infty$ Distance (Also called the Chebyshev Distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the $L_\\infty$ distance between the 15th and 25th row in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_l_inf_distance(p, q):\n",
    "    \"\"\"\n",
    "    Input: p and q are two numpy vectors of same dimensions. \n",
    "    Output: a single floating point value contaning the cosine distance between p and q. \n",
    "    \n",
    "    Allowed numpy functions: max, abs\n",
    "    \n",
    "    \"\"\"\n",
    "    ## BEGIN SOLUTION\n",
    "    return None\n",
    "    ## END SOLUTION\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_dist = calculate_l_inf_distance(iris_sk.data[15, :], iris_sk.data[25, :])\n",
    "l_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your function against the offical distance function implementations!\n",
    "np.testing.assert_almost_equal(l_dist,distance.chebyshev(iris_sk.data[15, :], iris_sk.data[25, :]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
