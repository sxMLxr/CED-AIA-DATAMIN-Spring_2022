{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 9: ANNs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Neural Network Playground (Follow: Explore individually; Discuss as a Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, go to Tensorflow's [Neural Network Playground](https://playground.tensorflow.org/). This website is an interactive and exploratory visualization of how the features, number of layers, training time, etc, influence the classification boundries of an ANN. Right now, we'll only worry ourselves with *classification* problems.\n",
    "\n",
    "Play with the visualization, and then answer the following questions below.\n",
    "\n",
    "### Scenarios\n",
    "\n",
    "1. Using the default network topology, try training the network with the different activation functions (ReLU, Tanh, Sigmoid, Linear). What effect does the activation function have on the training time? What effect does the activation function have on the shape of the classification boundries?\n",
    "2. Take a look at [this setup](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.21855&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). Train until the classification boundry converges. This is one of the rare cases where the nodes in an ANN can be (semi) interpreted. What do the nodes in the first hidden layer represent? What about the second hidden layer? How do you think the ANN uses these learned \"features\" to make a decision?\n",
    "\n",
    "### Exploration\n",
    "For each of the following questions:\n",
    "* Make a prediction before you begin exploring and testing.\n",
    "* Explain why you think this scenario has this property.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "3. Find a scenario where a simple model (fewer neurons) outperforms a complex model. (In regards to overfitting)\n",
    "4. Find a scenario where no hidden layers perform well.\n",
    "5. Find a scenario where a model with no hidden layers performs poorly no matter the features.\n",
    "6. Find a scenario where it takes a lot of training time to get a correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Answer]\n",
    "2. [Answer]\n",
    "3. [Answer]\n",
    "4. [Answer]\n",
    "5. [Answer]\n",
    "6. [Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Training and Testing a Neural Network (Group)\n",
    "\n",
    "For this problem, you'll be looking at a reduced subset of the [Credit Card Fraud Data](https://www.kaggle.com/mlg-ulb/creditcardfraud), which contains transactions made by credit cards in September 2013 by European cardholders, including some fradulent transactions.\n",
    " \n",
    "There are two interesting properties about this dataset:\n",
    "\n",
    "1) **The data only contains dimensionality reduced data from a PCA transformation.** Sometimes, due to privacy concerns, all of the features (and even the names of the features used) cannot be known. Therefore, you'll be trying to train a model of data that has been reduced in dimensions with uninterpretable features.\n",
    "\n",
    "2) **The dataset is highly unbalanced.** The positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "Knowing the data, what classification metrics (Precision, Recall, F1-Score) are most appropriate and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, **you have enough experience to do the entire model pipeline yourself**. That means *loading the data, creating splits, scaling the data, training and tuning the model, and evaluating the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the data into a dataframe. Use `value_counts` to check the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    85284\n",
       "1      158\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./creditcard.csv\")\n",
    "df.head\n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Partition the data into an X dataframe (features) and Y single-column dataframe (class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Time        V1        V2        V3        V4        V5        V6  \\\n",
      "0  164032.0  0.013026  0.777210  0.168464 -0.782449  0.631586 -0.531628   \n",
      "1   63407.0 -0.227828  0.503434  0.960992  0.979314  0.074042  0.640817   \n",
      "\n",
      "         V7        V8        V9  ...       V20       V21       V22       V23  \\\n",
      "0  0.876275 -0.000646 -0.248065  ... -0.035318 -0.226443 -0.515073  0.029329   \n",
      "1  0.374438  0.014293  0.091550  ...  0.124125 -0.102313 -0.032916 -0.353239   \n",
      "\n",
      "        V24       V25       V26       V27       V28  Amount  \n",
      "0 -0.409008 -0.497966  0.147070  0.244097  0.082947    4.49  \n",
      "1 -0.947066  0.137538  0.735928 -0.026360 -0.006919   74.50  \n",
      "\n",
      "[2 rows x 30 columns]\n",
      "0    0\n",
      "1    0\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "print(X.head(2))\n",
    "print(y.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create your train/test split. Use the provided random_state.\n",
    "\n",
    "**Note**: You should use a `train_size` of 0.7, or 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.7,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Use a [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to standardize the data. \n",
    "\n",
    "Fit the scaler only the the training X features, and then apply it to both training and test X features. We do this because in practice, we wouldn't be able to see data in the test X, so it shouldn't affect feature transformation. We therefore only use X_train for feature transformation.\n",
    "\n",
    "**Note**: Even though most of the features are already transformed using PCA (which would not require additional standardize), there is one other feature (time) that is not, so we should scale as a best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)  #good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Train an MLP with default hyperparameters.\n",
    "\n",
    "For the following, you'll be using sklearn's built in Multi-layer Perceptron classifier [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "Use the default hyperparams aside from `max_iter`. `max_iter` is how many iterations of training the ANN goes though until it manually stops. The default `max_iter=200` is too long for our data currently. \n",
    "\n",
    "**Use random_state as the random_states and max_iter=20**. The detault parameters will use a single hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(random_state=random_state, max_iter=20).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.99308789e-01 6.91211149e-04]\n",
      " [9.99708832e-01 2.91168266e-04]\n",
      " [9.99260322e-01 7.39678472e-04]\n",
      " ...\n",
      " [9.99536117e-01 4.63883191e-04]\n",
      " [9.99490107e-01 5.09892904e-04]\n",
      " [9.99615211e-01 3.84789155e-04]]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(clf.predict_proba(X_test))  #review some stats\n",
    "print(clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9993367924160262"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test, y_test)   #review some stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:  Evaluate the model on the test dataset using a confusion matrix and a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25582     3]\n",
      " [   14    34]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     25585\n",
      "           1       0.92      0.71      0.80        48\n",
      "\n",
      "    accuracy                           1.00     25633\n",
      "   macro avg       0.96      0.85      0.90     25633\n",
      "weighted avg       1.00      1.00      1.00     25633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, your model should have an accuracy of almost 100%. Use `classification_report` to explain what you think happened. Is the model performing well? If not, is it overfitting or underfitting? Remember that the classes in the problem are very imbalanced, but out main goal is to detect fraud (class 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `classification_report` outputs Precision, Recall and F1 for both classes. Remember that how we calculate these metrics depends on which class we treat as the positive class. If we say Class 0 is the positive class, a FP means incorrectly predicting Class 0, but for Class 1 a FP is incorrectly predicting Class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** The model is underfitting, classifying most data as class 0. We primarily care about Class 1, so we should use those metrics. The model has 89% precision Class 1, meaning we almost never have a \"false positive\" of incorrectly detecting fraud. However, for Class 1, the recall is only 60%, meaning almost half of frauds are not detected. So the classifier is not performing very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Hyperparameters (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparams**:\n",
    "\n",
    "ANNs have *a lot* of hyperparams. This can include simple things such as the number of layers and nodes, up to tuning the learning rate and the gradient descent algorithm used. \n",
    "\n",
    "Unfortunately, there is no tried an true method for selecting hyperparams for a neural network. It requires a lot of experimentation and intution through experience. (In fact, one of the most successful methods in training neural networks is *Graduate Student Descent*, where you simply give the laborious process of tuning to a graduate student while you go and do more research!)\n",
    "\n",
    "For now, the paramaters that you should explore are:\n",
    "\n",
    "* `activation`: The activation function of the the ANN. Defaults to ReLU.\n",
    "* `max_iter`: The ANN will train iterations until either the loss stops improving by a specified threshold, or `max_iters` is reached. Warning: the more you increase this, the more the training time will take! Patience is a virtue.\n",
    "* `hidden_layer_sizes`: A tuple representing the structure of the hidden layers. For example, giving the tuple `(100,50)` means that there's two hidden layers: the first being of size 100, and the second being of size 50. The tuple (100,) would mean a single hidden layer of size 100.\n",
    "\n",
    "**Try different permutations of these hyperprams and see how it affects the classification scores of your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25582     3]\n",
      " [   10    38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     25585\n",
      "           1       0.93      0.79      0.85        48\n",
      "\n",
      "    accuracy                           1.00     25633\n",
      "   macro avg       0.96      0.90      0.93     25633\n",
      "weighted avg       1.00      1.00      1.00     25633\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nclf = MLPClassifier(random_state=random_state, max_iter=45, hidden_layer_sizes=(200,200), activation='logistic').fit(X_train, y_train)\\nfrom sklearn.metrics import confusion_matrix\\nfrom sklearn.metrics import classification_report\\n\\ny_pred = clf.predict(X_test)\\nprint(confusion_matrix(y_test, y_pred))\\nprint(classification_report(y_test, y_pred))\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=random_state, max_iter=45, hidden_layer_sizes=(200,200), activation='relu').fit(X_train, y_train)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\"\"\"\n",
    "clf = MLPClassifier(random_state=random_state, max_iter=45, hidden_layer_sizes=(200,200), activation='logistic').fit(X_train, y_train)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "1. What criteria did you use to determine which model hyperparameters performed \"best\"? Why? Justify your answer with respect to the problem: fraud detection.\n",
    "2. What hyperparameters performed best. Why do you think they performed best?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
