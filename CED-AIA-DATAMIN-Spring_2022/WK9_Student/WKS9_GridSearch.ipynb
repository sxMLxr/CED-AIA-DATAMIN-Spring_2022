{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 9: ANNs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Neural Network Playground (Follow: Explore individually; Discuss as a Class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, go to Tensorflow's [Neural Network Playground](https://playground.tensorflow.org/). This website is an interactive and exploratory visualization of how the features, number of layers, training time, etc, influence the classification boundries of an ANN. Right now, we'll only worry ourselves with *classification* problems.\n",
    "\n",
    "Play with the visualization, and then answer the following questions below.\n",
    "\n",
    "### Scenarios\n",
    "\n",
    "1. Using the default network topology, try training the network with the different activation functions (ReLU, Tanh, Sigmoid, Linear). What effect does the activation function have on the training time? What effect does the activation function have on the shape of the classification boundries?\n",
    "2. Take a look at [this setup](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.21855&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). Train until the classification boundry converges. This is one of the rare cases where the nodes in an ANN can be (semi) interpreted. What do the nodes in the first hidden layer represent? What about the second hidden layer? How do you think the ANN uses these learned \"features\" to make a decision?\n",
    "\n",
    "### Exploration\n",
    "For each of the following questions:\n",
    "* Make a prediction before you begin exploring and testing.\n",
    "* Explain why you think this scenario has this property.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "3. Find a scenario where a simple model (fewer neurons) outperforms a complex model. (In regards to overfitting)\n",
    "4. Find a scenario where no hidden layers perform well.\n",
    "5. Find a scenario where a model with no hidden layers performs poorly no matter the features.\n",
    "6. Find a scenario where it takes a lot of training time to get a correct solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Answer]\n",
    "2. [Answer]\n",
    "3. [Answer]\n",
    "4. [Answer]\n",
    "5. [Answer]\n",
    "6. [Answer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Training and Testing a Neural Network (Group)\n",
    "\n",
    "For this problem, you'll be looking at a reduced subset of the [Credit Card Fraud Data](https://www.kaggle.com/mlg-ulb/creditcardfraud), which contains transactions made by credit cards in September 2013 by European cardholders, including some fradulent transactions.\n",
    " \n",
    "There are two interesting properties about this dataset:\n",
    "\n",
    "1) **The data only contains dimensionality reduced data from a PCA transformation.** Sometimes, due to privacy concerns, all of the features (and even the names of the features used) cannot be known. Therefore, you'll be trying to train a model of data that has been reduced in dimensions with uninterpretable features.\n",
    "\n",
    "2) **The dataset is highly unbalanced.** The positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "Knowing the data, what classification metrics (Precision, Recall, F1-Score) are most appropriate and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question, **you have enough experience to do the entire model pipeline yourself**. That means *loading the data, creating splits, scaling the data, training and tuning the model, and evaluating the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the data into a dataframe. Use `value_counts` to check the class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    85284\n",
       "1      158\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
       "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
       "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
       "       'Class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Partition the data into an X dataframe (features) and Y single-column dataframe (class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape\n",
      "(85442, 30)\n",
      "Class shape\n",
      "(85442,)\n"
     ]
    }
   ],
   "source": [
    "Features = df.drop(columns=['Class']).copy()\n",
    "\n",
    "Class = df['Class'].copy()\n",
    "\n",
    "print('Features shape')\n",
    "print(Features.shape)\n",
    "\n",
    "print('Class shape')\n",
    "print(Class.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create your train/test split. Use the provided random_state.\n",
    "\n",
    "**Note**: You should use a `train_size` of 0.7, or 70%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape\n",
      "(59809, 30)\n",
      "y_train shape\n",
      "(59809,)\n",
      "X_test shape\n",
      "(25633, 30)\n",
      "y_test shape\n",
      "(25633,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(Features,Class, train_size=0.7)\n",
    "\n",
    "print(\"X_train shape\")\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"y_train shape\")\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"X_test shape\")\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"y_test shape\")\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train value counts\n",
      "0    59700\n",
      "1      109\n",
      "Name: Class, dtype: int64\n",
      "\n",
      "\n",
      "y_test value counts\n",
      "0    25584\n",
      "1       49\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('y_train value counts')\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print('\\n\\ny_test value counts')\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Use a [MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to standardize the data. \n",
    "\n",
    "Fit the scaler only the the training X features, and then apply it to both training and test X features. We do this because in practice, we wouldn't be able to see data in the test X, so it shouldn't affect feature transformation. We therefore only use X_train for feature transformation.\n",
    "\n",
    "**Note**: Even though most of the features are already transformed using PCA (which would not require additional standardize), there is one other feature (time) that is not, so we should scale as a best practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape\n",
      "(59809, 30)\n",
      "y_train shape\n",
      "(59809,)\n",
      "X_test shape\n",
      "(25633, 30)\n",
      "y_test shape\n",
      "(25633,)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train shape\")\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"y_train shape\")\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"X_test shape\")\n",
    "print(X_test.shape)\n",
    "\n",
    "print(\"y_test shape\")\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5:  Train an MLP with default hyperparameters.\n",
    "\n",
    "For the following, you'll be using sklearn's built in Multi-layer Perceptron classifier [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
    "\n",
    "Use the default hyperparams aside from `max_iter`. `max_iter` is how many iterations of training the ANN goes though until it manually stops. The default `max_iter=200` is too long for our data currently. \n",
    "\n",
    "**Use random_state as the random_states and max_iter=20**. The detault parameters will use a single hidden layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9990246947294503"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MLPClassifier(random_state=random_state, max_iter=20).fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "print('Fit Score')\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6:  Evaluate the model on the test dataset using a confusion matrix and a classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "CONFUSION MATRIX:\n",
      "\u001b[0m[[25579     5]\n",
      " [   20    29]]\n",
      "\n",
      "\n",
      "\u001b[1mCLASSIFICATION REPORT\n",
      "\u001b[0m              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     25584\n",
      "           1       0.85      0.59      0.70        49\n",
      "\n",
      "    accuracy                           1.00     25633\n",
      "   macro avg       0.93      0.80      0.85     25633\n",
      "weighted avg       1.00      1.00      1.00     25633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m') \n",
    "print('CONFUSION MATRIX:')\n",
    "print('\\033[0m', end='')\n",
    "print(confusion_matrix(y_test, y_predict))\n",
    "\n",
    "print('\\n\\n\\033[1m', end='')\n",
    "print('CLASSIFICATION REPORT')\n",
    "print('\\033[0m', end='')\n",
    "print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, your model should have an accuracy of almost 100%. Use `classification_report` to explain what you think happened. Is the model performing well? If not, is it overfitting or underfitting? Remember that the classes in the problem are very imbalanced, but out main goal is to detect fraud (class 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: `classification_report` outputs Precision, Recall and F1 for both classes. Remember that how we calculate these metrics depends on which class we treat as the positive class. If we say Class 0 is the positive class, a FP means incorrectly predicting Class 0, but for Class 1 a FP is incorrectly predicting Class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer here**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Hyperparameters (Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparams**:\n",
    "\n",
    "ANNs have *a lot* of hyperparams. This can include simple things such as the number of layers and nodes, up to tuning the learning rate and the gradient descent algorithm used. \n",
    "\n",
    "Unfortunately, there is no tried an true method for selecting hyperparams for a neural network. It requires a lot of experimentation and intution through experience. (In fact, one of the most successful methods in training neural networks is *Graduate Student Descent*, where you simply give the laborious process of tuning to a graduate student while you go and do more research!)\n",
    "\n",
    "For now, the paramaters that you should explore are:\n",
    "\n",
    "* `activation`: The activation function of the the ANN. Defaults to ReLU.\n",
    "* `max_iter`: The ANN will train iterations until either the loss stops improving by a specified threshold, or `max_iters` is reached. Warning: the more you increase this, the more the training time will take! Patience is a virtue.\n",
    "* `hidden_layer_sizes`: A tuple representing the structure of the hidden layers. For example, giving the tuple `(100,50)` means that there's two hidden layers: the first being of size 100, and the second being of size 50. The tuple (100,) would mean a single hidden layer of size 100.\n",
    "\n",
    "**Try different permutations of these hyperprams and see how it affects the classification scores of your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=MLPClassifier(random_state=42),\n",
       "             param_grid={'activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
       "                         'hidden_layer_sizes': [(20,), (20, 20), (40,),\n",
       "                                                (40, 40)],\n",
       "                         'max_iter': [10, 15, 20]})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf2 = MLPClassifier(random_state=random_state).fit(X_train, y_train)\n",
    "\n",
    "parameters = {'activation':['identity','logistic','tanh','relu'], 'max_iter':[10,15,20],\\\n",
    "              'hidden_layer_sizes':[(20,),(20,20),(40,),(40,40)]}\n",
    "\n",
    "reg=GridSearchCV(clf2, parameters)\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.97240834, 1.40522275, 1.87312751, 1.23222466, 1.83389587,\n",
       "        2.45855446, 1.0572752 , 1.58173804, 2.04868426, 1.4437151 ,\n",
       "        2.09001245, 2.93011413, 1.26729445, 1.91077294, 2.2648222 ,\n",
       "        1.51448731, 2.36574721, 3.12684884, 1.29607105, 1.95819392,\n",
       "        2.6182847 , 1.96234269, 2.95452662, 3.98048501, 1.18071837,\n",
       "        1.79875851, 2.40244188, 1.77859812, 2.68349714, 3.46445675,\n",
       "        1.49906859, 2.12478976, 2.76118584, 2.0431253 , 3.1088387 ,\n",
       "        4.20351009, 1.13626199, 1.60160322, 2.14512978, 1.52837496,\n",
       "        2.30495305, 3.02778978, 1.24337058, 1.88983593, 2.60603523,\n",
       "        1.91862154, 2.70618057, 3.63070059]),\n",
       " 'std_fit_time': array([0.0607501 , 0.03419184, 0.09564896, 0.05468422, 0.00447507,\n",
       "        0.08041   , 0.02844369, 0.03065138, 0.05502144, 0.0438601 ,\n",
       "        0.05017969, 0.19048164, 0.03186314, 0.10393002, 0.07980433,\n",
       "        0.05493482, 0.07025839, 0.0299048 , 0.04709986, 0.07671117,\n",
       "        0.0596222 , 0.04875543, 0.06584968, 0.0411846 , 0.05687316,\n",
       "        0.03653665, 0.0890086 , 0.03772913, 0.05281968, 0.09285577,\n",
       "        0.09831316, 0.01100895, 0.03022169, 0.0874043 , 0.11140177,\n",
       "        0.11976403, 0.03991768, 0.03739742, 0.03423251, 0.07141622,\n",
       "        0.04934795, 0.05665291, 0.04779149, 0.06292096, 0.11864575,\n",
       "        0.15614909, 0.03263611, 0.03730077]),\n",
       " 'mean_score_time': array([0.00241265, 0.00403357, 0.00321403, 0.00323706, 0.00040345,\n",
       "        0.0024158 , 0.00040245, 0.00527663, 0.00566788, 0.00081172,\n",
       "        0.00723372, 0.00964904, 0.00805273, 0.00808868, 0.00926018,\n",
       "        0.01130233, 0.00803719, 0.01004882, 0.01166716, 0.00647531,\n",
       "        0.00607467, 0.01893625, 0.01574874, 0.02039475, 0.00844841,\n",
       "        0.009267  , 0.00683804, 0.01612916, 0.01448169, 0.01046877,\n",
       "        0.00643516, 0.01167727, 0.00682869, 0.01857495, 0.01727867,\n",
       "        0.01814008, 0.00403008, 0.002421  , 0.00362005, 0.00641069,\n",
       "        0.00804892, 0.004036  , 0.00200481, 0.00765057, 0.00362778,\n",
       "        0.00648584, 0.00564513, 0.00684686]),\n",
       " 'std_score_time': array([0.00388542, 0.004225  , 0.00393637, 0.00396457, 0.0008069 ,\n",
       "        0.00390307, 0.0008049 , 0.0035423 , 0.00386087, 0.00099421,\n",
       "        0.00272621, 0.00075738, 0.00311223, 0.00314098, 0.00099407,\n",
       "        0.00348472, 0.00648218, 0.00637436, 0.00321401, 0.00449997,\n",
       "        0.00425741, 0.0072578 , 0.00795979, 0.00796702, 0.00601121,\n",
       "        0.00100907, 0.0035097 , 0.00492417, 0.00468124, 0.00078207,\n",
       "        0.00447893, 0.00320613, 0.00666875, 0.00713102, 0.0037435 ,\n",
       "        0.00755524, 0.00423105, 0.00391375, 0.00447826, 0.00462262,\n",
       "        0.00309946, 0.00423943, 0.00400963, 0.00296214, 0.00449126,\n",
       "        0.00451586, 0.00391349, 0.00393129]),\n",
       " 'param_activation': masked_array(data=['identity', 'identity', 'identity', 'identity',\n",
       "                    'identity', 'identity', 'identity', 'identity',\n",
       "                    'identity', 'identity', 'identity', 'identity',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic',\n",
       "                    'logistic', 'logistic', 'logistic', 'logistic', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh',\n",
       "                    'tanh', 'tanh', 'tanh', 'tanh', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu', 'relu', 'relu', 'relu', 'relu', 'relu',\n",
       "                    'relu', 'relu'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_hidden_layer_sizes': masked_array(data=[(20,), (20,), (20,), (20, 20), (20, 20), (20, 20),\n",
       "                    (40,), (40,), (40,), (40, 40), (40, 40), (40, 40),\n",
       "                    (20,), (20,), (20,), (20, 20), (20, 20), (20, 20),\n",
       "                    (40,), (40,), (40,), (40, 40), (40, 40), (40, 40),\n",
       "                    (20,), (20,), (20,), (20, 20), (20, 20), (20, 20),\n",
       "                    (40,), (40,), (40,), (40, 40), (40, 40), (40, 40),\n",
       "                    (20,), (20,), (20,), (20, 20), (20, 20), (20, 20),\n",
       "                    (40,), (40,), (40,), (40, 40), (40, 40), (40, 40)],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_iter': masked_array(data=[10, 15, 20, 10, 15, 20, 10, 15, 20, 10, 15, 20, 10, 15,\n",
       "                    20, 10, 15, 20, 10, 15, 20, 10, 15, 20, 10, 15, 20, 10,\n",
       "                    15, 20, 10, 15, 20, 10, 15, 20, 10, 15, 20, 10, 15, 20,\n",
       "                    10, 15, 20, 10, 15, 20],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'activation': 'identity',\n",
       "   'hidden_layer_sizes': (20,),\n",
       "   'max_iter': 10},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (20,), 'max_iter': 15},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (20,), 'max_iter': 20},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (20, 20), 'max_iter': 10},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (20, 20), 'max_iter': 15},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (20, 20), 'max_iter': 20},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (40,), 'max_iter': 10},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (40,), 'max_iter': 15},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (40,), 'max_iter': 20},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (40, 40), 'max_iter': 10},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (40, 40), 'max_iter': 15},\n",
       "  {'activation': 'identity', 'hidden_layer_sizes': (40, 40), 'max_iter': 20},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (20,), 'max_iter': 10},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (20,), 'max_iter': 15},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (20,), 'max_iter': 20},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (20, 20), 'max_iter': 10},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (20, 20), 'max_iter': 15},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (20, 20), 'max_iter': 20},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (40,), 'max_iter': 10},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (40,), 'max_iter': 15},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (40,), 'max_iter': 20},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (40, 40), 'max_iter': 10},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (40, 40), 'max_iter': 15},\n",
       "  {'activation': 'logistic', 'hidden_layer_sizes': (40, 40), 'max_iter': 20},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (20,), 'max_iter': 10},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (20,), 'max_iter': 15},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (20,), 'max_iter': 20},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (20, 20), 'max_iter': 10},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (20, 20), 'max_iter': 15},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (20, 20), 'max_iter': 20},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (40,), 'max_iter': 10},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (40,), 'max_iter': 15},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (40,), 'max_iter': 20},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (40, 40), 'max_iter': 10},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (40, 40), 'max_iter': 15},\n",
       "  {'activation': 'tanh', 'hidden_layer_sizes': (40, 40), 'max_iter': 20},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (20,), 'max_iter': 10},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (20,), 'max_iter': 15},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (20,), 'max_iter': 20},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (20, 20), 'max_iter': 10},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (20, 20), 'max_iter': 15},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (20, 20), 'max_iter': 20},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (40,), 'max_iter': 10},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (40,), 'max_iter': 15},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (40,), 'max_iter': 20},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (40, 40), 'max_iter': 10},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (40, 40), 'max_iter': 15},\n",
       "  {'activation': 'relu', 'hidden_layer_sizes': (40, 40), 'max_iter': 20}],\n",
       " 'split0_test_score': array([0.99816084, 0.99824444, 0.99816084, 0.99832804, 0.99882963,\n",
       "        0.99891323, 0.99824444, 0.99841164, 0.99891323, 0.99841164,\n",
       "        0.99882963, 0.99874603, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99816084, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99816084, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99824444, 0.99824444, 0.99891323, 0.99908042,\n",
       "        0.99824444, 0.99841164, 0.99899682, 0.99874603, 0.99891323,\n",
       "        0.99891323, 0.99816084, 0.99816084, 0.99832804, 0.99841164,\n",
       "        0.99891323, 0.99908042, 0.99824444, 0.99832804, 0.99891323,\n",
       "        0.99882963, 0.99899682, 0.99899682]),\n",
       " 'split1_test_score': array([0.99816084, 0.99841164, 0.99857883, 0.99857883, 0.99899682,\n",
       "        0.99899682, 0.99849523, 0.99891323, 0.99908042, 0.99899682,\n",
       "        0.99899682, 0.99899682, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99816084, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99816084, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99857883, 0.99832804, 0.99949841, 0.99949841,\n",
       "        0.99832804, 0.99891323, 0.99916402, 0.99899682, 0.99949841,\n",
       "        0.99933122, 0.99816084, 0.99857883, 0.99866243, 0.99857883,\n",
       "        0.99949841, 0.99958201, 0.99832804, 0.99857883, 0.99908042,\n",
       "        0.99916402, 0.99958201, 0.99958201]),\n",
       " 'split2_test_score': array([0.99816084, 0.99816084, 0.99866243, 0.99908042, 0.99949841,\n",
       "        0.99958201, 0.99857883, 0.99916402, 0.99949841, 0.99933122,\n",
       "        0.99916402, 0.99949841, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99816084, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99816084, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99882963, 0.99849523, 0.99966561, 0.9998328 ,\n",
       "        0.99849523, 0.99916402, 0.99958201, 0.99958201, 0.99916402,\n",
       "        0.99941481, 0.99816084, 0.99849523, 0.99899682, 0.99832804,\n",
       "        0.9998328 , 0.99974921, 0.99849523, 0.99899682, 0.99949841,\n",
       "        0.9998328 , 0.99933122, 0.99949841]),\n",
       " 'split3_test_score': array([0.99816084, 0.99832804, 0.99874603, 0.99891323, 0.99908042,\n",
       "        0.99924762, 0.99857883, 0.99899682, 0.99916402, 0.99908042,\n",
       "        0.99916402, 0.99916402, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99816084, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99816084, 0.99816084, 0.99816084, 0.99816084,\n",
       "        0.99816084, 0.99882963, 0.99866243, 0.99916402, 0.99916402,\n",
       "        0.99849523, 0.99899682, 0.99916402, 0.99908042, 0.99916402,\n",
       "        0.99916402, 0.99849523, 0.99882963, 0.99891323, 0.99891323,\n",
       "        0.99916402, 0.99924762, 0.99849523, 0.99882963, 0.99899682,\n",
       "        0.99924762, 0.99924762, 0.99924762]),\n",
       " 'split4_test_score': array([0.99824429, 0.9984115 , 0.99857871, 0.99866232, 0.99899674,\n",
       "        0.99908034, 0.9984115 , 0.99882953, 0.99891313, 0.99891313,\n",
       "        0.99899674, 0.99899674, 0.99824429, 0.99824429, 0.99824429,\n",
       "        0.99824429, 0.99824429, 0.99824429, 0.99824429, 0.99824429,\n",
       "        0.99824429, 0.99824429, 0.99824429, 0.99824429, 0.99824429,\n",
       "        0.99824429, 0.99866232, 0.9984115 , 0.99899674, 0.99924755,\n",
       "        0.99824429, 0.99882953, 0.99891313, 0.99899674, 0.99916395,\n",
       "        0.99941476, 0.9984115 , 0.99882953, 0.99882953, 0.99866232,\n",
       "        0.99916395, 0.99916395, 0.9984115 , 0.99882953, 0.99882953,\n",
       "        0.99899674, 0.99941476, 0.99941476]),\n",
       " 'mean_test_score': array([0.99817753, 0.99831129, 0.99854537, 0.99871257, 0.9990804 ,\n",
       "        0.999164  , 0.99846177, 0.99886305, 0.99911384, 0.99894665,\n",
       "        0.99903025, 0.9990804 , 0.99817753, 0.99817753, 0.99817753,\n",
       "        0.99817753, 0.99817753, 0.99817753, 0.99817753, 0.99817753,\n",
       "        0.99817753, 0.99817753, 0.99817753, 0.99817753, 0.99817753,\n",
       "        0.99817753, 0.99862897, 0.99842833, 0.9992476 , 0.99936464,\n",
       "        0.99836145, 0.99886305, 0.999164  , 0.9990804 , 0.99918073,\n",
       "        0.99924761, 0.99827785, 0.99857881, 0.99874601, 0.99857881,\n",
       "        0.99931448, 0.99936464, 0.99839489, 0.99871257, 0.99906368,\n",
       "        0.99921416, 0.99931449, 0.99934793]),\n",
       " 'std_test_score': array([3.33805148e-05, 9.74639355e-05, 2.02019914e-04, 2.62241151e-04,\n",
       "        2.24323389e-04, 2.36456460e-04, 1.25128778e-04, 2.51353491e-04,\n",
       "        2.15434060e-04, 3.01882471e-04, 1.25122608e-04, 2.47997581e-04,\n",
       "        3.33805148e-05, 3.33805148e-05, 3.33805148e-05, 3.33805148e-05,\n",
       "        3.33805148e-05, 3.33805148e-05, 3.33805148e-05, 3.33805148e-05,\n",
       "        3.33805148e-05, 3.33805148e-05, 3.33805148e-05, 3.33805148e-05,\n",
       "        3.33805148e-05, 3.33805148e-05, 2.15413666e-04, 1.43830639e-04,\n",
       "        2.89606705e-04, 2.72694022e-04, 1.13428234e-04, 2.51353491e-04,\n",
       "        2.30483725e-04, 2.74736367e-04, 1.86182978e-04, 1.90624327e-04,\n",
       "        1.45733834e-04, 2.47972135e-04, 2.36444106e-04, 2.04763466e-04,\n",
       "        3.18996464e-04, 2.56862475e-04, 9.74867134e-05, 2.34064788e-04,\n",
       "        2.32896922e-04, 3.41025206e-04, 1.93538432e-04, 2.07481777e-04]),\n",
       " 'rank_test_score': array([34, 32, 27, 23, 13, 10, 28, 19, 12, 18, 17, 13, 34, 34, 34, 34, 34,\n",
       "        34, 34, 34, 34, 34, 34, 34, 34, 34, 24, 29,  7,  1, 31, 19, 11, 13,\n",
       "         9,  6, 33, 25, 21, 26,  5,  2, 30, 22, 16,  8,  4,  3])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_activation</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_hidden_layer_sizes</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>identity</td>\n",
       "      <td>10</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>identity</td>\n",
       "      <td>15</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>identity</td>\n",
       "      <td>20</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>identity</td>\n",
       "      <td>10</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.998713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>identity</td>\n",
       "      <td>15</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.999080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>identity</td>\n",
       "      <td>20</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.999164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>identity</td>\n",
       "      <td>10</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.998462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>identity</td>\n",
       "      <td>15</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.998863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>identity</td>\n",
       "      <td>20</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.999114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>identity</td>\n",
       "      <td>10</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.998947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>identity</td>\n",
       "      <td>15</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.999030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>identity</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.999080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>logistic</td>\n",
       "      <td>15</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>logistic</td>\n",
       "      <td>20</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>logistic</td>\n",
       "      <td>15</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>logistic</td>\n",
       "      <td>20</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>logistic</td>\n",
       "      <td>15</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>logistic</td>\n",
       "      <td>20</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>logistic</td>\n",
       "      <td>10</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>logistic</td>\n",
       "      <td>15</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>logistic</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>tanh</td>\n",
       "      <td>15</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>tanh</td>\n",
       "      <td>20</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.998428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>tanh</td>\n",
       "      <td>15</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.999248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>tanh</td>\n",
       "      <td>20</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.999365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.998361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>tanh</td>\n",
       "      <td>15</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.998863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>tanh</td>\n",
       "      <td>20</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.999164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>tanh</td>\n",
       "      <td>10</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.999080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>tanh</td>\n",
       "      <td>15</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.999181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>tanh</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.999248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>relu</td>\n",
       "      <td>15</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>relu</td>\n",
       "      <td>20</td>\n",
       "      <td>(20,)</td>\n",
       "      <td>0.998746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.998579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>relu</td>\n",
       "      <td>15</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.999314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>relu</td>\n",
       "      <td>20</td>\n",
       "      <td>(20, 20)</td>\n",
       "      <td>0.999365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.998395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>relu</td>\n",
       "      <td>15</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.998713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>relu</td>\n",
       "      <td>20</td>\n",
       "      <td>(40,)</td>\n",
       "      <td>0.999064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>relu</td>\n",
       "      <td>10</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.999214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>relu</td>\n",
       "      <td>15</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.999314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>relu</td>\n",
       "      <td>20</td>\n",
       "      <td>(40, 40)</td>\n",
       "      <td>0.999348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   param_activation param_max_iter param_hidden_layer_sizes  mean_test_score\n",
       "0          identity             10                    (20,)         0.998178\n",
       "1          identity             15                    (20,)         0.998311\n",
       "2          identity             20                    (20,)         0.998545\n",
       "3          identity             10                 (20, 20)         0.998713\n",
       "4          identity             15                 (20, 20)         0.999080\n",
       "5          identity             20                 (20, 20)         0.999164\n",
       "6          identity             10                    (40,)         0.998462\n",
       "7          identity             15                    (40,)         0.998863\n",
       "8          identity             20                    (40,)         0.999114\n",
       "9          identity             10                 (40, 40)         0.998947\n",
       "10         identity             15                 (40, 40)         0.999030\n",
       "11         identity             20                 (40, 40)         0.999080\n",
       "12         logistic             10                    (20,)         0.998178\n",
       "13         logistic             15                    (20,)         0.998178\n",
       "14         logistic             20                    (20,)         0.998178\n",
       "15         logistic             10                 (20, 20)         0.998178\n",
       "16         logistic             15                 (20, 20)         0.998178\n",
       "17         logistic             20                 (20, 20)         0.998178\n",
       "18         logistic             10                    (40,)         0.998178\n",
       "19         logistic             15                    (40,)         0.998178\n",
       "20         logistic             20                    (40,)         0.998178\n",
       "21         logistic             10                 (40, 40)         0.998178\n",
       "22         logistic             15                 (40, 40)         0.998178\n",
       "23         logistic             20                 (40, 40)         0.998178\n",
       "24             tanh             10                    (20,)         0.998178\n",
       "25             tanh             15                    (20,)         0.998178\n",
       "26             tanh             20                    (20,)         0.998629\n",
       "27             tanh             10                 (20, 20)         0.998428\n",
       "28             tanh             15                 (20, 20)         0.999248\n",
       "29             tanh             20                 (20, 20)         0.999365\n",
       "30             tanh             10                    (40,)         0.998361\n",
       "31             tanh             15                    (40,)         0.998863\n",
       "32             tanh             20                    (40,)         0.999164\n",
       "33             tanh             10                 (40, 40)         0.999080\n",
       "34             tanh             15                 (40, 40)         0.999181\n",
       "35             tanh             20                 (40, 40)         0.999248\n",
       "36             relu             10                    (20,)         0.998278\n",
       "37             relu             15                    (20,)         0.998579\n",
       "38             relu             20                    (20,)         0.998746\n",
       "39             relu             10                 (20, 20)         0.998579\n",
       "40             relu             15                 (20, 20)         0.999314\n",
       "41             relu             20                 (20, 20)         0.999365\n",
       "42             relu             10                    (40,)         0.998395\n",
       "43             relu             15                    (40,)         0.998713\n",
       "44             relu             20                    (40,)         0.999064\n",
       "45             relu             10                 (40, 40)         0.999214\n",
       "46             relu             15                 (40, 40)         0.999314\n",
       "47             relu             20                 (40, 40)         0.999348"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESULTS = pd.DataFrame(columns=['param_activation','param_max_iter','param_hidden_layer_sizes','mean_test_score'])\n",
    "\n",
    "RESULTS['param_activation'] = reg.cv_results_['param_activation']\n",
    "RESULTS['param_max_iter'] = reg.cv_results_['param_max_iter']\n",
    "RESULTS['param_hidden_layer_sizes'] = reg.cv_results_['param_hidden_layer_sizes']\n",
    "RESULTS['mean_test_score'] = reg.cv_results_['mean_test_score']\n",
    "\n",
    "RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', hidden_layer_sizes=(20, 20), max_iter=20,\n",
      "              random_state=42)\n"
     ]
    }
   ],
   "source": [
    "best_estimator = reg.best_estimator_\n",
    "print(best_estimator)\n",
    "\n",
    "Predicted_test = best_estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPredicted test shape\n",
      "\u001b[0m(25633,)\n",
      "\n",
      "\n",
      "\u001b[1my_test shape\n",
      "\u001b[0m(25633,)\n",
      "\n",
      "\n",
      "\u001b[1mCONFUSION MATRIX:\n",
      "\u001b[0m[[25578     6]\n",
      " [   12    37]]\n",
      "\n",
      "\n",
      "\u001b[1mCLASSIFICATION REPORT\n",
      "\u001b[0m              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     25584\n",
      "           1       0.86      0.76      0.80        49\n",
      "\n",
      "    accuracy                           1.00     25633\n",
      "   macro avg       0.93      0.88      0.90     25633\n",
      "weighted avg       1.00      1.00      1.00     25633\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m', end = '') \n",
    "print('Predicted test shape')\n",
    "print('\\033[0m', end = '')\n",
    "print(Predicted_test.shape)\n",
    "\n",
    "print('\\n\\n\\033[1m', end = '')\n",
    "print('y_test shape')\n",
    "print('\\033[0m', end = '')\n",
    "print(y_test.shape)\n",
    "\n",
    "print('\\n\\n\\033[1m', end = '')\n",
    "print('CONFUSION MATRIX:')\n",
    "print('\\033[0m', end = '')\n",
    "print(confusion_matrix(y_test, Predicted_test))\n",
    "\n",
    "print('\\n\\n\\033[1m', end = '')\n",
    "print('CLASSIFICATION REPORT')\n",
    "print('\\033[0m', end = '')\n",
    "print(classification_report(y_test, Predicted_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Class'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.scatterplot(x=y_test,y=Predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Class'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Predicted_train = best_estimator.predict(X_train)\n",
    "sns.scatterplot(x=y_train,y=Predicted_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the following questions:\n",
    "1. What criteria did you use to determine which model hyperparameters performed \"best\"? Why? Justify your answer with respect to the problem: fraud detection.\n",
    "2. What hyperparameters performed best. Why do you think they performed best?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
